---
layout: post
title: "大模型部署技术"
subtitle: ""
author: "NiuHuLu"
published: true
header-style: text
tags:
  - LLM
  - 部署
---

# 大模型框架/平台对比与适用场景

本文对比了以下大模型相关框架和平台，并给出了各自适合的应用场景：

- Ollama
- vLLM
- SGLang
- FastChat
- LLaMA Factory
- One-API
- LocalAI

---

## 1. 框架概览

| 框架/平台 | 类型 | 特点 | 适合场景 | 优势 | 局限 |
|------------|------|------|----------|------|------|
| **Ollama** | 本地 & 云端 LLM 管理 | 专注 Mac/Windows 本地运行大模型，提供简单 API 接口 | 个人开发者、原型验证、快速本地测试 | 轻量、易用，开箱即用 | 模型生态相对有限，对 GPU 扩展有限 |
| **vLLM** | 高性能推理库 | 针对大模型的高吞吐量推理优化 | 多 GPU 分布式推理、高并发服务 | 高性能，低延迟，支持 tensor parallel | 仅推理，不管理模型生命周期 |
| **SGLang** | 多模型统一接口框架 | 提供统一接口对接不同模型 | 企业级应用、嵌入式服务 | 多模型兼容性强，便于迁移 | 社区较小，文档资源有限 |
| **FastChat** | LLM 聊天系统框架 | 提供聊天机器人系统、消息管理、模型调度 | 构建本地或云端聊天系统 | 成熟的聊天系统实现，支持多模型切换 | 主要专注聊天，不通用推理或 embedding |
| **LLaMA Factory** | 模型管理与推理平台 | 对 LLaMA 系列模型做封装和管理 | 模型训练/推理快速部署 | 易于管理 LLaMA 模型，提供统一接口 | 仅针对 LLaMA 系列 |
| **One-API** | 模型接口统一平台 | 类似 OpenAI API 的统一调用接口 | 企业多模型服务，SaaS 应用 | 统一接口调用不同模型，支持 embedding/chat | 依赖 One-API 平台，可能有限制 |
| **LocalAI** | 本地部署 LLM 服务器 | 本地部署 OpenAI 兼容接口，支持多种模型 | 本地推理、离线环境、隐私敏感场景 | 支持 OpenAI API 协议，方便替换 | 模型管理需自己处理，部署成本高 |

---

## 2. 深入对比

### 2.1 推理性能
- **vLLM** → 专注高性能推理，低延迟，高吞吐量，支持分布式。
- **FastChat / Ollama / LocalAI** → 中等性能，适合单机或小规模并发。
- **SGLang / One-API / LLaMA Factory** → 侧重接口和管理，不是推理优化工具。

### 2.2 模型管理
- **Ollama / LLaMA Factory** → 本地模型管理方便。
- **One-API / SGLang** → 多模型统一管理接口，适合企业。
- **vLLM** → 只关注推理，不管理模型。

### 2.3 部署场景

| 场景 | 推荐框架/平台 |
|------|---------------|
| 个人本地测试 | Ollama, LocalAI |
| 企业多模型统一调用 | SGLang, One-API |
| 高并发聊天机器人 | FastChat + vLLM |
| LLaMA 系列专用部署 | LLaMA Factory |
| 离线部署，隐私敏感 | LocalAI |

### 2.4 功能对比（API / Chat / Embedding）

| 框架/平台 | Chat | Embedding | Training | 推理优化 |
|------------|------|-----------|----------|-----------|
| Ollama | ✅ | ❌ | ❌ | ❌ |
| vLLM | ❌ | ❌ | ❌ | ✅ |
| SGLang | ✅ | ✅ | ❌ | ❌ |
| FastChat | ✅ | ❌ | ❌ | ⚠️（多模型调度） |
| LLaMA Factory | ⚠️ | ⚠️ | ✅（微调） | ⚠️ |
| One-API | ✅ | ✅ | ❌ | ❌ |
| LocalAI | ✅ | ✅ | ❌ | ⚠️ |

> ⚠️ 表示功能部分支持或需自定义。

---

## 3. 总结

1. **本地快速原型** → Ollama, LocalAI  
2. **高并发推理** → vLLM  
3. **聊天系统** → FastChat + vLLM  
4. **企业级统一接口** → SGLang, One-API  
5. **LLaMA 系列优化** → LLaMA Factory  

---


# fastchat
启动一个openai接口
```
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```