---
layout: post
title: "å¤§æ¨¡å‹éƒ¨ç½²æŠ€æœ¯"
subtitle: ""
author: "NiuHuLu"
published: true
header-style: text
tags:
  - LLM
  - éƒ¨ç½²
---
# å¤§æ¨¡å‹æ¡†æ¶/å¹³å°å¯¹æ¯”ä¸é€‚ç”¨åœºæ™¯

æœ¬æ–‡å¯¹æ¯”äº†ä»¥ä¸‹å¤§æ¨¡å‹ç›¸å…³æ¡†æ¶å’Œå¹³å°ï¼Œå¹¶ç»™å‡ºäº†å„è‡ªé€‚åˆçš„åº”ç”¨åœºæ™¯ï¼š

- Ollama
- vLLM
- SGLang
- FastChat
- LLaMA Factory
- One-API
- LocalAI

---

## 1. æ¡†æ¶æ¦‚è§ˆ

| æ¡†æ¶/å¹³å°               | ç±»å‹                 | ç‰¹ç‚¹                                               | é€‚åˆåœºæ™¯                           | ä¼˜åŠ¿                                      | å±€é™                                 |
| ----------------------- | -------------------- | -------------------------------------------------- | ---------------------------------- | ----------------------------------------- | ------------------------------------ |
| **Ollama**        | æœ¬åœ° & äº‘ç«¯ LLM ç®¡ç† | ä¸“æ³¨ Mac/Windows æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹ï¼Œæä¾›ç®€å• API æ¥å£ | ä¸ªäººå¼€å‘è€…ã€åŸå‹éªŒè¯ã€å¿«é€Ÿæœ¬åœ°æµ‹è¯• | è½»é‡ã€æ˜“ç”¨ï¼Œå¼€ç®±å³ç”¨                      | æ¨¡å‹ç”Ÿæ€ç›¸å¯¹æœ‰é™ï¼Œå¯¹ GPU æ‰©å±•æœ‰é™    |
| **vLLM**          | é«˜æ€§èƒ½æ¨ç†åº“         | é’ˆå¯¹å¤§æ¨¡å‹çš„é«˜ååé‡æ¨ç†ä¼˜åŒ–                       | å¤š GPU åˆ†å¸ƒå¼æ¨ç†ã€é«˜å¹¶å‘æœåŠ¡      | é«˜æ€§èƒ½ï¼Œä½å»¶è¿Ÿï¼Œæ”¯æŒ tensor parallel      | ä»…æ¨ç†ï¼Œä¸ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ           |
| **SGLang**        | å¤šæ¨¡å‹ç»Ÿä¸€æ¥å£æ¡†æ¶   | æä¾›ç»Ÿä¸€æ¥å£å¯¹æ¥ä¸åŒæ¨¡å‹                           | ä¼ä¸šçº§åº”ç”¨ã€åµŒå…¥å¼æœåŠ¡             | å¤šæ¨¡å‹å…¼å®¹æ€§å¼ºï¼Œä¾¿äºè¿ç§»                  | ç¤¾åŒºè¾ƒå°ï¼Œæ–‡æ¡£èµ„æºæœ‰é™               |
| **FastChat**      | LLM èŠå¤©ç³»ç»Ÿæ¡†æ¶     | æä¾›èŠå¤©æœºå™¨äººç³»ç»Ÿã€æ¶ˆæ¯ç®¡ç†ã€æ¨¡å‹è°ƒåº¦             | æ„å»ºæœ¬åœ°æˆ–äº‘ç«¯èŠå¤©ç³»ç»Ÿ             | æˆç†Ÿçš„èŠå¤©ç³»ç»Ÿå®ç°ï¼Œæ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢        | ä¸»è¦ä¸“æ³¨èŠå¤©ï¼Œä¸é€šç”¨æ¨ç†æˆ– embedding |
| **LLaMA Factory** | æ¨¡å‹ç®¡ç†ä¸æ¨ç†å¹³å°   | å¯¹ LLaMA ç³»åˆ—æ¨¡å‹åšå°è£…å’Œç®¡ç†                      | æ¨¡å‹è®­ç»ƒ/æ¨ç†å¿«é€Ÿéƒ¨ç½²              | æ˜“äºç®¡ç† LLaMA æ¨¡å‹ï¼Œæä¾›ç»Ÿä¸€æ¥å£         | ä»…é’ˆå¯¹ LLaMA ç³»åˆ—                    |
| **One-API**       | æ¨¡å‹æ¥å£ç»Ÿä¸€å¹³å°     | ç±»ä¼¼ OpenAI API çš„ç»Ÿä¸€è°ƒç”¨æ¥å£                     | ä¼ä¸šå¤šæ¨¡å‹æœåŠ¡ï¼ŒSaaS åº”ç”¨          | ç»Ÿä¸€æ¥å£è°ƒç”¨ä¸åŒæ¨¡å‹ï¼Œæ”¯æŒ embedding/chat | ä¾èµ– One-API å¹³å°ï¼Œå¯èƒ½æœ‰é™åˆ¶        |
| **LocalAI**       | æœ¬åœ°éƒ¨ç½² LLM æœåŠ¡å™¨  | æœ¬åœ°éƒ¨ç½² OpenAI å…¼å®¹æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹             | æœ¬åœ°æ¨ç†ã€ç¦»çº¿ç¯å¢ƒã€éšç§æ•æ„Ÿåœºæ™¯   | æ”¯æŒ OpenAI API åè®®ï¼Œæ–¹ä¾¿æ›¿æ¢            | æ¨¡å‹ç®¡ç†éœ€è‡ªå·±å¤„ç†ï¼Œéƒ¨ç½²æˆæœ¬é«˜       |

---

## 2. æ·±å…¥å¯¹æ¯”æ¦‚æ‹¬

### 2.1 æ¨ç†æ€§èƒ½

- **vLLM** â†’ ä¸“æ³¨é«˜æ€§èƒ½æ¨ç†ï¼Œä½å»¶è¿Ÿï¼Œé«˜ååé‡ï¼Œæ”¯æŒåˆ†å¸ƒå¼ã€‚
- **FastChat / Ollama / LocalAI** â†’ ä¸­ç­‰æ€§èƒ½ï¼Œé€‚åˆå•æœºæˆ–å°è§„æ¨¡å¹¶å‘ã€‚
- **SGLang / One-API / LLaMA Factory** â†’ ä¾§é‡æ¥å£å’Œç®¡ç†ï¼Œä¸æ˜¯æ¨ç†ä¼˜åŒ–å·¥å…·ã€‚

### 2.2 æ¨¡å‹ç®¡ç†

- **Ollama / LLaMA Factory** â†’ æœ¬åœ°æ¨¡å‹ç®¡ç†æ–¹ä¾¿ã€‚
- **One-API / SGLang** â†’ å¤šæ¨¡å‹ç»Ÿä¸€ç®¡ç†æ¥å£ï¼Œé€‚åˆä¼ä¸šã€‚
- **vLLM** â†’ åªå…³æ³¨æ¨ç†ï¼Œä¸ç®¡ç†æ¨¡å‹ã€‚

### 2.3 éƒ¨ç½²åœºæ™¯

| åœºæ™¯               | æ¨èæ¡†æ¶/å¹³å°   |
| ------------------ | --------------- |
| ä¸ªäººæœ¬åœ°æµ‹è¯•       | Ollama, LocalAI |
| ä¼ä¸šå¤šæ¨¡å‹ç»Ÿä¸€è°ƒç”¨ | SGLang, One-API |
| é«˜å¹¶å‘èŠå¤©æœºå™¨äºº   | FastChat + vLLM |
| LLaMA ç³»åˆ—ä¸“ç”¨éƒ¨ç½² | LLaMA Factory   |
| ç¦»çº¿éƒ¨ç½²ï¼Œéšç§æ•æ„Ÿ | LocalAI         |

### 2.4 åŠŸèƒ½å¯¹æ¯”ï¼ˆAPI / Chat / Embeddingï¼‰

| æ¡†æ¶/å¹³å°     | Chat | Embedding | Training   | æ¨ç†ä¼˜åŒ–           |
| ------------- | ---- | --------- | ---------- | ------------------ |
| Ollama        | âœ…   | âŒ        | âŒ         | âŒ                 |
| vLLM          | âŒ   | âŒ        | âŒ         | âœ…                 |
| SGLang        | âœ…   | âœ…        | âŒ         | âŒ                 |
| FastChat      | âœ…   | âŒ        | âŒ         | âš ï¸ï¼ˆå¤šæ¨¡å‹è°ƒåº¦ï¼‰ |
| LLaMA Factory | âš ï¸ | âš ï¸      | âœ…ï¼ˆå¾®è°ƒï¼‰ | âš ï¸               |
| One-API       | âœ…   | âœ…        | âŒ         | âŒ                 |
| LocalAI       | âœ…   | âœ…        | âŒ         | âš ï¸               |

> âš ï¸ è¡¨ç¤ºåŠŸèƒ½éƒ¨åˆ†æ”¯æŒæˆ–éœ€è‡ªå®šä¹‰ã€‚

---

## 3. æ€»ç»“

1. **æœ¬åœ°å¿«é€ŸåŸå‹** â†’ Ollama, LocalAI
2. **é«˜å¹¶å‘æ¨ç†** â†’ vLLM
3. **èŠå¤©ç³»ç»Ÿ** â†’ FastChat + vLLM
4. **ä¼ä¸šçº§ç»Ÿä¸€æ¥å£** â†’ SGLang, One-API
5. **LLaMA ç³»åˆ—ä¼˜åŒ–** â†’ LLaMA Factory

## vllmè°ƒç”¨æ—¶å€™æ¨¡å‹èµ„æº

### èµ„æºåˆ†æ

| ç»„æˆéƒ¨åˆ†                             | å ç”¨è¯´æ˜                                         | æ˜¯å¦å— `gpu_memory_utilization` æ§åˆ¶ |
| ------------------------------------ | ------------------------------------------------ | -------------------------------------- |
| 1. æ¨¡å‹æƒé‡ï¼ˆModel Weightsï¼‰         | æ¨¡å‹å‚æ•°ï¼ˆfloat16/bfloat16ï¼‰ï¼Œå¸¸é©»æ˜¾å­˜           | âŒ ä¸å—æ§åˆ¶ï¼ˆå¿…é¡»åŠ è½½ï¼‰                |
| 2. KV Cacheï¼ˆPagedAttention ç¼“å­˜ï¼‰   | å­˜å‚¨æ¯ä¸ª token çš„ key/valueï¼Œéš batch å’Œé•¿åº¦å¢é•¿ | âœ… å— `gpu_memory_utilization` æ§åˆ¶  |
| 3. ä¸´æ—¶å·¥ä½œç©ºé—´ï¼ˆTemporary Buffersï¼‰ | Attention è®¡ç®—ã€softmaxã€é‡‡æ ·ç­‰ä¸­é—´ç»“æœ          | âŒ ä¸å—æ§åˆ¶ï¼ŒåŠ¨æ€åˆ†é…                  |
| 4. æ˜¾å­˜ç¢ç‰‡ & å¯¹é½å¼€é”€               | CUDA åˆ†é…å™¨éœ€è¦å¯¹é½å—ï¼Œå¯èƒ½äº§ç”Ÿç¢ç‰‡              | âŒ æ— æ³•ç²¾ç¡®é¢„æµ‹                        |
| 5. ç³»ç»Ÿ/é©±åŠ¨/å…¶ä»–è¿›ç¨‹                | NVIDIA é©±åŠ¨ã€æ¡Œé¢ GUIã€ç›‘æ§å·¥å…·ç­‰                | âŒ ä¸å¯æ§                              |

### å®ä¾‹è¯´æ˜

ä¸¾ä¸ªå®é™…ä¾‹å­ï¼ˆä»¥24G GPUä¸ºä¾‹ï¼‰
å‡è®¾ï¼š
    æ¨¡å‹ï¼šQwen-1.7B  çº¦3.4GB
    è®¾ç½®ï¼š gpu_memory_utilization = 0.8
    æ€»æ˜¾å­˜ï¼š24G

```
æ€»æ˜¾å­˜ï¼š24 GB
â”‚
â”œâ”€â”€ æ¨¡å‹æƒé‡ï¼š3.4 GBï¼ˆå›ºå®šï¼‰
â”œâ”€â”€ KV Cache æœ€å¤šï¼š24 Ã— 0.8 = 19.2 GB â†’ å®é™…å¯ç”¨ â‰ˆ 19.2 - 3.4 = 15.8 GBï¼Ÿ
â””â”€â”€ å‰©ä½™ 20%ï¼ˆ4.8 GBï¼‰ï¼šç•™ç»™ä¸´æ—¶è®¡ç®— + å®‰å…¨è¾¹é™…
```

### vllmå‚æ•°è§£æ

- batch_size

  - 'å–‚é¥­çš„é€Ÿåº¦â€™
- `max_num_seqs`
  - vllmå†…éƒ¨çš„æ¶ˆåŒ–èƒ½åŠ›
  
- `gpu_memory_utilization`

  - å€¼è¶Šå¤§ â†’ èƒ½ç¼“å­˜æ›´å¤šè¯·æ±‚çš„ä¸­é—´çŠ¶æ€ â†’ æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡æˆ–æ›´é«˜å¹¶å‘
  - å€¼å¤ªå° â†’ æµªè´¹æ˜¾å­˜ï¼Œé™ä½åå
  - å€¼å¤ªå¤§ â†’ å¯èƒ½ OOMï¼ˆOut-Of-Memoryï¼‰

| å‚æ•°                              | å«ä¹‰                           | æ§åˆ¶å†…å®¹                                                | ä½œç”¨å¯¹è±¡     | æ˜¯å¦åŒ…å« Prompt    | å½±å“èµ„æº                    | å…¸å‹å€¼           |
| --------------------------------- | ------------------------------ | ------------------------------------------------------- | ------------ | ------------------ | --------------------------- | ---------------- |
| `max_seq_lenï¼ˆå…¶ä»–æ¡†æ¶ï¼ŒéLLM)` | æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¸Šä¸‹æ–‡çª—å£ä¸Šé™ï¼‰ | å•æ¡è¯·æ±‚ä¸­**prompt + ç”Ÿæˆå†…å®¹** çš„æ€» token æ•°ä¸Šé™ | å•æ¡è¯·æ±‚     | âœ… æ˜¯              | æ˜¾å­˜ï¼ˆKV Cache éšé•¿åº¦å¢é•¿ï¼‰ | 2048, 4096, 8192 |
| `max_tokens`                    | æœ€å¤§ç”Ÿæˆ token æ•°              | å•æ¡è¯·æ±‚ä¸­**æœ€å¤šç”Ÿæˆçš„æ–° token æ•°**               | å•æ¡è¯·æ±‚     | âŒ å¦ï¼ˆä»…è¾“å‡ºï¼‰    | æ˜¾å­˜ + æ¨ç†æ—¶é•¿             | 128, 256, 512    |
| `max_num_seqs(VLLMä¸­)`          | æœ€å¤§å¹¶å‘åºåˆ—æ•°                 | ä¸€ä¸ª batch ä¸­**æœ€å¤šåŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡**           | æ•´ä¸ªæ¨ç†æ‰¹æ¬¡ | â€”ï¼ˆæ¯æ¡ç‹¬ç«‹è®¡ç®—ï¼‰ | æ€»æ˜¾å­˜ + ååé‡             | 16, 32, 64, 256  |



| å‚æ•°                       | å«ä¹‰                                       | ä½œç”¨é˜¶æ®µ                  | å…¸å‹å€¼                          | å¤‡æ³¨                                                       |
| -------------------------- | ------------------------------------------ | ------------------------- | ------------------------------- | ---------------------------------------------------------- |
| `max_model_len`          | æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆinput + outputï¼‰ | é™åˆ¶å•ä¸ªè¯·æ±‚çš„æœ€å¤§é•¿åº¦    | `4096`,`32768`,`"auto"`   |                                                            |
| `max_num_batched_tokens` | æ‰€æœ‰å¹¶å‘è¯·æ±‚çš„æ€» token æ•°ä¸Šé™              | æ§åˆ¶ GPU æ˜¾å­˜ä½¿ç”¨ï¼Œé˜² OOM | `32768`,`262144`,`524288` | `max_num_batched_tokens â‰ˆ max_num_seqs Ã— max_model_len`` |

### è°ƒç”¨ä»£ç 

```python
import sys
import time
sys.path.append("")

from cal_acc_lora_llm import evaluate_predictions,evaluate_predictions_relaxed_v2
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import pandas as pd
import torch
import time
from tqdm import tqdm
import re
import os
from loguru import logger
import json



key_map = {
    "æ€åº¦": "attitude",
    "éªšæ‰°-ä¸å†è‡´ç”µ": "no_call",
    "éªšæ‰°-å¤–å‘¼é¢‘æ¬¡": "freq_call",
    "æŠ•è¯‰å€¾å‘": "complaint"
}

ID_TO_LABEL = {
    "0": "å…¶ä»–",
    "4": "éªšæ‰°-ä¸å†è‡´ç”µ",
    "5": "æ€åº¦",
    "6": "éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "7": "æŠ•è¯‰å€¾å‘",
    "8": "æŠ•è¯‰å€¾å‘&éªšæ‰°-ä¸å†è‡´ç”µ",
    "9": "æŠ•è¯‰å€¾å‘&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "10": "æ€åº¦&æŠ•è¯‰å€¾å‘",
    "11": "æ€åº¦&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "12": "æ€åº¦&éªšæ‰°-ä¸å†è‡´ç”µ",
    "13": "éªšæ‰°-ä¸å†è‡´ç”µ&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "14": "æ€åº¦&éªšæ‰°-ä¸å†è‡´ç”µ&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "15": "æ€åº¦&æŠ•è¯‰å€¾å‘&éªšæ‰°-ä¸å†è‡´ç”µ",
    "16": "æ€åº¦&æŠ•è¯‰å€¾å‘&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡",
    "17": "æŠ•è¯‰å€¾å‘&éªšæ‰°-ä¸å†è‡´ç”µ&éªšæ‰°-å¤–å‘¼é¢‘æ¬¡"
}

def normalize_label(label: str) -> str:
    return "&".join(sorted(label.split("&")))

# key_mapçš„è‹±æ–‡ -> ä¸­æ–‡çš„æ˜ å°„
eng2zh = {v: k for k, v in key_map.items()}

# ID_TO_LABEL çš„è‹±æ–‡ --> label -> id åæŸ¥
LABEL_TO_ID = {
    normalize_label(v): k
    for k, v in ID_TO_LABEL.items()
}


def pred_emotion_to_label(pred_emotion):
    """
    è¾“å…¥:
      - dict
      - æˆ– json å­—ç¬¦ä¸²
    è¾“å‡º:
      - (label_id, label_name)
    """
    if pd.isna(pred_emotion):
        return pd.Series(["0", "å…¶ä»–"])

    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆååºåˆ—åŒ–
    if isinstance(pred_emotion, str):
        try:
            pred_emotion = json.loads(pred_emotion)
        except (json.JSONDecodeError, TypeError) as e:
            # è§£æå¤±è´¥ï¼šè§†ä¸ºæ— å‘½ä¸­ï¼Œè¿”å›â€œå…¶ä»–â€
            # å¯é€‰ï¼šè®°å½•æ—¥å¿—æˆ–æ‰“å°é”™è¯¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"JSON è§£æå¤±è´¥: {e} | åŸå§‹è¾“å…¥: {repr(pred_emotion)}")
            pred_emotion = {"attitude": 0, "no_call": 0, "freq_call": 0, "complaint": 0}

    # å‘½ä¸­çš„ä¸­æ–‡æ ‡ç­¾
    hit_labels = [eng2zh[k] for k, v in pred_emotion.items() if v == 1]

    if not hit_labels:
        label_name = "å…¶ä»–"
    else:
        # æ’åºä¿è¯å’Œ ID_TO_LABEL å®Œå…¨ä¸€è‡´
        label_name = normalize_label("&".join(hit_labels))

    pred_label = LABEL_TO_ID.get(label_name, "0")
    return pd.Series([pred_label, label_name])


def run_inference(
    model_path: str,
    input_excel_path: str,
    output_excel_path: str,
    system_prompt: str,
    batch_size: int = 32,
    dtype: str = "bfloat16",
    gpu_memory_utilization: float = 0.87,
    max_num_seqs: int = 32,
    temperature: float = 0.5,
    top_p: float = 0.9,
    repetition_penalty: float = 1.1,
    max_tokens: int = 2048,
    is_label: bool = True,
    sheet_name:str = ''
):
    """
    ä½¿ç”¨ vLLM å¯¹ Excel ä¸­çš„æ–‡æœ¬è¿›è¡Œæ‰¹é‡æƒ…ç»ªåˆ†ç±»æ¨ç†ã€‚

    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆåˆå¹¶ LoRA åçš„ checkpointï¼‰
        input_excel_path: è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å« 'text_a' åˆ—ï¼‰
        output_excel_path: è¾“å‡º Excel æ–‡ä»¶è·¯å¾„ï¼ˆä¼šæ–°å¢ 'pred_emotion' åˆ—ï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆæƒ…ç»ªåˆ†ç±»è§„åˆ™ï¼‰
        batch_size: æ¨ç†æ‰¹å¤§å°
        dtype: æ¨¡å‹ç²¾åº¦ï¼ˆå¦‚ 'bfloat16', 'float16'ï¼‰
        gpu_memory_utilization: GPU æ˜¾å­˜åˆ©ç”¨ç‡
        max_num_seqs: vLLM æœ€å¤§å¹¶å‘åºåˆ—æ•°
        temperature / top_p / repetition_penalty / max_tokens: é‡‡æ ·å‚æ•°
    """
    logger.info("ğŸ”§ æ­£åœ¨åŠ è½½ tokenizer å’Œæ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    llm = LLM(
        model=model_path,
        dtype=dtype,
        gpu_memory_utilization=gpu_memory_utilization,
        trust_remote_code=True,
        max_num_seqs=max_num_seqs,
    )

    logger.info("ğŸ“¥ æ­£åœ¨è¯»å–è¾“å…¥æ•°æ®...")
    if not sheet_name :
        sheet_name = 0
    df = pd.read_excel(input_excel_path, sheet_name=sheet_name)
    # df = df.head()
    logger.info("ğŸ“ æ­£åœ¨æ„å»º prompts...")
    prompts = []
    for _, row in df.iterrows():
        user_query = str(row["text_a"]) if pd.notna(row["text_a"]) else ""
        messages = [
            {"role": "system", "content": system_prompt.strip()},
            {"role": "user", "content": user_query},
        ]
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        prompts.append(prompt)

    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        max_tokens=max_tokens,
    )

    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†ï¼ˆbatch_size={batch_size}ï¼‰...")
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start = time.time()

    results = []
    for i in tqdm(range(0, len(prompts), batch_size)):
        batch_prompts = prompts[i : i + batch_size]
        outputs = llm.generate(batch_prompts, sampling_params=sampling_params)
        for out in outputs:
            text = out.outputs[0].text.strip()
            # ç§»é™¤ <think>...</think> æ ‡ç­¾ï¼ˆå¦‚æœæ¨¡å‹è¾“å‡ºä¸­æœ‰ï¼‰
            cleaned_text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
            results.append(cleaned_text.strip())

    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end = time.time()
    if is_label:
        df["pred_label"] = results
    else:
        df["pred_emotion"] = results

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_excel_path), exist_ok=True)
    df.to_excel(output_excel_path, index=False)

    logger.info(f"\nâœ… å…¨éƒ¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(results)} æ¡ã€‚")
    logger.info(
        f"âš™ï¸ æ€»æ¨ç†è€—æ—¶: {end - start:.2f} ç§’ï¼Œå¹³å‡æ¯æ¡ {(end - start) / len(results):.2f} ç§’ã€‚"
    )




if __name__ == "__main__":
    # ==============================
    # è·¯å¾„ä¸é…ç½®ï¼ˆé›†ä¸­ç®¡ç†ï¼‰
    # ==============================
    from prompt import *


     # TODO å›å¤´å¯¹äºæŸä¸€ä¸ªæ¨¡å‹ï¼Œç›´æ¥ç»™ä¸€ä¸ªåˆ—è¡¨
    BASE_PATH = '/data/oceanus_share/mzh/inspection_dianxiao_inference/test/resource/benchmark_data/'
    MODEL_BASE_PATH = "/data/oceanus_share/mzh/LLaMA-Factory/saves/Qwen3-1.7B-Base/merge_lora"
    PRE_PATH = "/data/oceanus_share/mzh/inspection_dianxiao_inference/test/resource/benchmark_data_pre/"
    test_date = ''

    # --------------------------éœ€è¦é…ç½® ----------------------------
    sheet_name = "standard_test_set2"                                                      # æµ‹è¯•é›†ç‰ˆæœ¬
    version = "stress_test"                                                         # æ•°æ®ç‰ˆæœ¬
    model_path ='train_2026-01-08-23-04-01/checkpoint-840'                    # æ¨¡å‹é…ç½®
    lora_rank = 16
    SYSTEM_PROMPT = QIFU_NOTHINK_V12                                           #æç¤ºè¯é…ç½®

    # -------------------------------------------------------------------

    model_display_path = model_path.replace("/checkpoint","-checkpoint")
    INPUT_EXCEL =os.path.join(BASE_PATH,test_date,f"{sheet_name}.xlsx")
    MODEL_PATH = os.path.join(MODEL_BASE_PATH,model_path)
    OUTPUT_EXCEL = os.path.join(PRE_PATH,f"{version}/{sheet_name}_{model_display_path}_{lora_rank}.xlsx")
    END_OUTPUT_EXCEL = OUTPUT_EXCEL.replace(".xlsx","_end.xlsx")

    # # è°ƒç”¨æ¨ç†å‡½æ•°
    run_inference(
        model_path=MODEL_PATH,
        input_excel_path=INPUT_EXCEL,
        output_excel_path=OUTPUT_EXCEL,
        system_prompt=SYSTEM_PROMPT,
        batch_size=400,
        dtype="float16",
        gpu_memory_utilization=0.7,              # åªæ§åˆ¶kv cache çš„ç¼“å­˜
        max_num_seqs=512,
        temperature=0.01,
        top_p=1.0,
        repetition_penalty=1.0,
        max_tokens=2048,
        is_label = False,
    )

    data = pd.read_excel(OUTPUT_EXCEL)
    logger.info(f"ä¿å­˜åˆ°{OUTPUT_EXCEL}")
    if "attitude" in  data["pred_emotion"][0]:
        data[["pred_label", "label_name"]]=data["pred_emotion"].apply(pred_emotion_to_label)
        data = data.dropna(subset=['true_label'])
        data.to_excel(END_OUTPUT_EXCEL,index=False)

    # æ¨ç†è¿‡ç¨‹
    OUTPUT_WITH_LABEL_FILE = END_OUTPUT_EXCEL
    METRICS_OUTPUT_FILE = END_OUTPUT_EXCEL.replace("benchmark_data_pre","benchmark_data_metric")
    METRICS_OUTPUT_FILE = METRICS_OUTPUT_FILE.replace(f"{version}/",f"{version}/metric_")

    MUL_METRICS_OUTPUT_FILE = END_OUTPUT_EXCEL.replace("benchmark_data_pre","benchmark_data_metric")
    MUL_METRICS_OUTPUT_FILE = MUL_METRICS_OUTPUT_FILE.replace(f"{version}/",f"{version}/mul_metric_")
    print(f"ä¿å­˜åˆ° {MUL_METRICS_OUTPUT_FILE}")
    evaluate_predictions(
        input_excel_path=END_OUTPUT_EXCEL,
        output_excel_with_pred_path=OUTPUT_WITH_LABEL_FILE,
        metrics_output_path=METRICS_OUTPUT_FILE,
        is_label=True,  # æ³¨æ„ï¼šå¦‚æœ pred_emotion æ˜¯æ–‡æœ¬ï¼Œåº”è®¾ä¸º False
    )

    evaluate_predictions_relaxed_v2(
        input_excel_path=END_OUTPUT_EXCEL,
        output_excel_with_pred_path=OUTPUT_WITH_LABEL_FILE,
        metrics_output_path=MUL_METRICS_OUTPUT_FILE,
        is_label=True,  # æ³¨æ„ï¼šå¦‚æœ pred_emotion æ˜¯æ–‡æœ¬ï¼Œåº”è®¾ä¸º False
    )





```

## fastchat

å¯åŠ¨ä¸€ä¸ªopenaiæ¥å£

```
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```
