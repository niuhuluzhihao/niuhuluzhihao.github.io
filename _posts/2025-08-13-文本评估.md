---
layout: post
title: "大模型评估"
date: 2025-08-13 23:10:00
author: "NiuHuLu"
catalog: false
published: true
header-style: text
tags:
  - 大模型
---
# 大模型评估调研

## 评估方法

- 主观评估（专家评估）
- 客观评估
  - BLEU
  - ROUGE
- LLM评测

## 构建LLM评测

![此图和下文可能存在差异，但问题不大](../pict/大模型知识/评估/评估系统.png)

输入设计、模型选择和训练，以及输出后处理---评估

- 输入设计和提示设计
  - 对于输入设计，需要考虑要评估的变量类型（如文本、图像或视频）、输入方式（例如单独、成对或批量）及其位置（例如在开头、中间或结尾）。对于提示设计，可以采用四种不同的方法。这些方法包括生成分数、解决真/假问题、进行成对比较和进行多项选择。
- 模型选择
  - 外部现有大模型
  - 微调模型
    - 数据收集
      - 指令、待评估对象和评估结果(更大模型打标或者人工打标）
    - 提示
    - 模型微调
- 后处理（评估格式和输入设计一样）
  - 提取特定标记
    - 保证每个输入和最终评估相同（比如，需要修改吗，需要和不需要，yes和no）
    - constrained decoding
  - 归一化输出 logits
  - 选择高回报的句子
- 模型评估
  - 用于模型
  - 用于数据
    - 评估生成高质量数据
  - 用于代理
  - 用于推理或者思考

## 改进

![](../pict/大模型知识/评估/改进.png)
LLM存在固有的长度偏差，具体性偏差和知识偏差

- 评估提示的设计策略（基于情境学习）
  - fewshot
  - 拆解评估任务
  - 成对任务的时候会出现位置偏差（裁判会倾向于左边跑道或者右边跑道，多次评分）
  - 格式化输出
- LLMs 评估能力的改进策略（基于模型）
  - 微调评测模型
  - 评估迭代优化（基于更大的模型或者人工评估结果评测）
- 最终评估结果的优化策略（基于后处理）
  - 整合多个结果
    - 调整温度和超参数
    - 使用多个LLM（多个模型融合）
  - 优化LLM输出
    - 自我反思
      2. 第一次评估： LLM对答案A和B进行评估，输出结果（例如“A更好”）。
      3. 自我验证： 我们向同一个LLM出示一个新的提示，包含：
      1. 原始问题
      2. 答案A和B
      3. 它自己刚才给出的评估结果
      4. 然后询问：“你对这个判断结果有多确定？”或者“请检查这个判断是否合理？”
      5. 过滤结果：
      6. 如果LLM表示“非常确定”或认为判断合理，我们就保留这个评估结果。
    - 评分平滑（窥探内心）
      FLEUR的评分平滑策略是如何工作的？
      假设我们让LLM在一个0-9的尺度上打分。

2. 显式评分： LLM最终输出了数字“7”。
3. 窥探内心： 我们不去看最终的“7”，而是去看在生成这个分数时，模型为所有可能数字（0,1,2,...,9）对应的token所计算出的概率。
4. 可能的情况是：P(‘6’ token) = 0.25， P(‘7’ token) = 0.60， P(‘8’ token) = 0.15
5. 平滑计算： 我们不直接采用“7”，而是用这些概率作为权重，计算一个加权平均分。
6. 最终评分 = (6 * 0.25) + (7 * 0.60) + (8 * 0.15) = 6.9

- 评估任务逐点评估转为成对比较

## 评估评估器优劣

![](../pict/大模型知识/评估/评估优劣.png)
LLM存在幻觉，导致评估结果不理想。

### 基础指标

一致性(依赖LLM生成和人类判断的数据集,所以需要构建）
sum（人工=机器）/总数据集
kappa
spearmans
准确率，召回率，F1分数

### 偏见

任务无关
特定于判断的偏差

### 鲁棒性

## 实验

![](../pict/大模型知识/评估/实验.png)

### 评估维度和基准

人工对齐
llmeval  https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/WideDeep
偏差
EVALBIASBENCH  https://github.com/ncsoft/offsetbias-

### 评估指标

#### 一致性

用于对齐人工

#### 准确率

除位置偏差歪歪的偏差，使用准确率进行评估，表示LLM作为评判者选择EVALBIASBENCH 中标注的正确候选回复样本的比例。

#### 位置一致性

交互评估顺序，认为两次打分是否相同

- 将所有 N 个样本的指示函数返回值相加，得到总共有多少个样本的评估结果是一致的。
- 将这个总和除以总样本数 N，得到一个介于 0 和 1 之间的比率，这就是位置一致性。

## 基准和指标（评估指标）

基准就是模型在特定任务下的数据集
![](../pict/大模型知识/评估/评估范围.png)

### 评估的任务分类：

| 任务          | 类型                | 核心能力                     | 是否生成            | 典型用途               |
| ------------- | ------------------- | ---------------------------- | ------------------- | ---------------------- |
| ELI5          | 长文本问答/解释生成 | 知识整合、简化表达、连贯生成 | 是                  | 开放域问答、教育AI     |
| ROCStories    | 故事理解/选择       | 常识推理、叙事连贯性         | 否（原始为选择）    | 常识推理评测           |
| Self-Instruct | 指令数据合成        | 指令理解与生成、泛化         | 是（生成指令+响应） | 模型对齐、微调数据构建 |

核心能力：

- 推理能力
  - 逻辑
  - 数学
  - 常识
  - 多跳
  - 结构化数据推理
- 社会影响
  - 安全性
  - 可信度
- 特定领域知识
  - 金融

## LLM的功能

性能评估

模型增强

数据构建

![](../pict/大模型知识/评估/LLM_as_judges功能.png)

## 其他

![](../pict/大模型知识/评估/评估概况1.png)
![](../pict/大模型知识/评估/评估概况2.png)

## 一些思路详细设计

1、人机协同CoEval --**AI 先想标准 → 人类审核 → AI 打分 → 人类再审**

![1768806058450](image/2025-08-13-文本评估/1768806058450.png)

### 回复评估

 self-rag 检索，生成和**评测**

    训练**Critic**模型（提高检索的准确性，而不是一次性丢给模型）--- > 模型在训练中学会生成特殊标记，如“[RETRIEVE]”、“[RELEVANT]”、“[SUPPORTED]”等，这些标记是模型“自我反思”的体现，使其在推理阶段具备可控性。

![1768808410948](image/2025-08-13-文本评估/1768808410948.png)

### 模型评估

模型评估通常分为两种类型：

* 静态数据评估
* 多轮对话交互评估

![1768809992311](image/2025-08-13-文本评估/1768809992311.png)

## 当前已有思路

1、多模型评估
2、多模型评估的结果--训练模型--进行评估。（只运行一个模型，不用多模型，提高效率） ---- https://arxiv.org/abs/2412.05579（构建多轮对话数据集用于训练）
3、意图切换下的多轮对话评估 --https://arxiv.org/pdf/2505.20451
   AMULET 框架。该框架借助对话行为（dialog-acts）和准则（maxims）这两个关键语言学概念，提升了 LLM 评判模型在含复杂多轮对话语境的偏好数据上的评判准确性。AMULET 能提供两类极具价值的洞察：（a）对话中存在的交际结构与意图（即对话行为）；（b）偏好回复对会话原则（即准则）的满足程度，并基于这些洞察做出评判。

## 参考

https://arxiv.org/abs/2412.05579
https://github.com/onejune2018/Awesome-LLM-Eval
https://arxiv.org/abs/2411.15594
https://dl.acm.org/doi/full/10.1145/3641289
https://zhuanlan.zhihu.com/p/18593955786
https://github.com/MLGroupJLU/LLM-eval-survey
