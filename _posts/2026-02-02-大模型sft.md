# 技术

## 数据

### 数据生成

#### 人工构建

人工直接构建

#### 蒸馏数据生成

用更大的模型蒸馏模型

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=MTY0ZTYxNzJjYzg5OTdiNTg2ODllOTA2ZGNjMDc3NDVfc2ZWdzJCU3NReVFDOUpnVmluRFBzZEtEdFJuck5XNjZfVG9rZW46RTZSWmJENnpPb0Y2TTl4YmRiSmM2SHlhbjBjXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

#### 自我改进的生成

依赖强大的LLM做生成数据

##### self-instruct

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=YWU1MWY2YTFkYTkyYjE5N2MzMjA2ODIxODdhMmQ5OTJfU2w5ZlJIVlVIaVZEcmpVZmU2UUsyZ0g0c1VZRVcybnZfVG9rZW46Sjk1cWJoNDNVb3RnbDJ4VUtoM2NmYVM2bkJmXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

  **self_instruct示例过程**    初始准备：构建任务池

* 首先，我们有 175 个 “种子任务”，每个任务包含 1 条指令和 1 个示例（输入 - 输出对）。
* 这些种子任务被存入 “任务池”（Task Pool），作为整个流程的起点数据。

  **Step 1：指令生成（Instruction Generation）**
* 任务池里的种子任务被送入大语言模型（LM）。
* LM 会基于这些种子任务，生成全新的、多样化的任务指令。例如：“Give me a quote from a famous person on this topic.”（给我一句名人关于这个主题的名言）

  **Step 2：分类任务识别（Classification Task Identification）**
* 新生成的指令再次送入 LM，由它判断这个指令对应的任务是否为 **分类任务** 。
* 这个判断会决定后续生成示例的模式：

  * 如果是分类任务 → 进入 “Output-first” 模式
  * 如果不是分类任务 → 进入 “Input-first” 模式

    **Step 3：示例生成（Instance Generation）**

    这一步会根据上一步的判断，生成完整的任务示例（输入 - 输出对）：
* **Output-first 模式（分类任务）**

  * LM 先确定一个类别标签（比如 “Pro-abortion”）
  * 再生成符合该标签的输入文本（比如 “I believe that women should have the right to choose whether or not they want to have an abortion.”）
  * 最终形成 “指令 + 类别标签 + 输入文本” 的完整示例。
* **Input-first 模式（非分类任务）**

  * LM 先生成输入（比如 “Topic: The importance of being honest.”）
  * 再根据输入生成对应的输出（比如 “"Honesty is the first chapter in the book of wisdom." - Thomas Jefferson”）
  * 最终形成 “指令 + 输入 + 输出” 的完整示例。

    **Step 4：过滤（Filtering）**
* 所有生成的任务示例会被送入过滤模块，检查其质量、合理性和有效性。
* 只有通过过滤的高质量示例会被添加回 **任务池** ，让任务池的规模和多样性不断扩大。
* 这个循环会持续进行，实现数据的自我迭代与增长。

##### self-alignment

自增强

1、用种子数据训练逆向模型Myx，利用回复---生成输入

2、利用Myx---生成训练数据

自筛选

1、用种子数据初始模型M0

2、用M0筛选高质量数据

3、用种子数据+M0生成数据训练M1，继续生成高质量数据

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=MTQ3M2ZlNWUwOTFkODMzOTkzZGZiNjdmNWJkOGQ1NDRfdktzR2dYb09EeWhlRHJpT1ZWdFNNVTE1SDFxcFZTSDRfVG9rZW46Q29ld2JPWmJob3gwWXR4MzM1YWM2VXVGbnNiXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

**self-alignment示例说明**

   **场景：** 我们的目标是训练一个能理解并回答 “美食制作” 类指令的模型。

  **step0：初始化数据**

* **种子数据** （人工标注）：只有 3 条高质量（指令，输出）对
* 指令：“怎么做番茄炒蛋？” → 输出：“先把番茄切块，鸡蛋打散，热锅倒油炒鸡蛋盛出，再炒番茄出汁，最后混合鸡蛋加盐调味即可。”
* 指令：“可乐鸡翅的做法？” → 输出：“鸡翅焯水后煎至两面金黄，加入可乐、生抽、姜片，小火焖煮 15 分钟收汁即可。”
* 指令：“如何煮出溏心蛋？” → 输出：“水开后放入鸡蛋，煮 6-7 分钟，捞出放入冰水冷却，剥壳即可。”

   **无标注数据（只有输出）** ：从美食论坛爬取的 1000 条食谱文本，比如：* “土豆去皮切条，泡水去淀粉，油温六成热下锅炸至金黄，复炸一次更酥脆。”

* “虾仁用料酒腌制 10 分钟，西兰花焯水，蒜末爆香后炒虾仁，加入西兰花翻炒加盐出锅。”

  ** step 1：自增强（Self-Augmentation）**    我们先用那 3 条种子数据微调基础模型（LLaMA），得到一个**指令预测模型** Myx。

  把无标注数据里的输出喂给 Myx，让它反向生成对应的指令：1.       输入：“土豆去皮切条，泡水去淀粉，油温六成热下锅炸至金黄，复炸一次更酥脆。”

1. Myx 生成的指令：“怎么做炸薯条？”
2. 输入：“虾仁用料酒腌制 10 分钟，西兰花焯水，蒜末爆香后炒虾仁，加入西兰花翻炒加盐出锅。”
3. Myx 生成的指令：“西兰花炒虾仁的做法？”

   这样，我们就得到了 1000 条候选增强数据 A，每条都是（生成的指令，原始输出）的配对。

  **Step 2：自筛选（Self-Curation）与迭代**

  第一轮迭代（Iteration 1）    先用 3 条种子数据微调得到初始模型 M0。

    用 M0 去评估那 1000 条候选数据的质量。比如，它会判断 “怎么做炸薯条？” 和对应的薯条做法是否匹配。

    筛选出质量最高的 200 条，组成子集 Ak(1)。

  第二轮迭代（Iteration 2）

  用 Ak(1) 这 200 条数据去微调 M0，得到能力更强的模型 M1。    用 M1 重新评估剩下的 800 条候选数据，这次筛选出质量更高的 300 条，组成 Ak(2)。

    再用 Ak(2) 微调得到 M2，以此类推。

  **最终效果**

  经过几轮迭代后，模型 M2 已经见过了几百条高质量的（指令，输出）对，它的指令理解和生成能力会比只用 3 条种子数据训练的 M0 强得多。

##### Self-play

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2E5NjdkMmNjYmQ3OTY0OWRiZmZlYmU1NzNkMTQ2NjZfUXdlTGxhaVltQ3EwMTllNDJNcW55UHU1UkNUZ3V3UXdfVG9rZW46UlpnRWJ3UDVYb2ZwNFd4Wlc1WGNyUE96bnFmXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

**self-play示例说明**

  数据生成过程

1. **初始状态** ：从一个已经过有监督微调（SFT）的“弱”模型 *pθt* 开始，并使用现有的 SFT 数据集（包含提示词 *x* 和人类回答  *y* ）,。
2. **合成响应生成** ：

   ◦ 模型扮演“对手玩家”的角色，针对 SFT 数据集中的提示词  *x* ，由模型 *pθt* 自己生成相应的回答  *y* ′,。

   ◦ 例如，在实验中，研究者从 **Ultrachat200k** 数据集中随机抽取了 50k 个提示词，并让模型生成对应的合成响应。
3. **构建对比对** ：

   ◦ 将**人类标注的原始回答（Ground Truth）**标记为“优选（Chosen）”,。

   ◦ 将**模型在当前迭代中自己生成的回答**标记为“拒绝（Rejected）”,。
4. **模型更新（自我博弈）** ：

   ◦ 新模型  *pθt* +1（主玩家）的目标是学会辨别这两个回答。它通过优化损失函数， **提高生成人类回答的概率，同时降低生成自己上一轮合成回答的概率** ,。

   ◦ 训练目标是让模型生成的分布 *pθ* 尽可能接近目标数据分布  *pdata* ，直到连最强的模型也无法分辨两者,。
5. **迭代演进** ：新训练的模型在下一轮迭代中变成“对手玩家”，继续生成新的合成响应，如此循环往复,。

---

  具体示例说明

  文章通过**“南安普顿通勤方式”**的案例直观地展示了这一过程：

  •  **提示词 (Prompt)** ：“在南安普顿，通勤者最常用的交通工具是什么？”

  •  **人类正确答案 (Ground Truth)** ：一个诚实的回答，提到由于无法获取最新数据，但历史上巴士最受欢迎，同时也有火车和出租车网络。

  •  **迭代 0 的模型表现（生成训练数据）** ：

    ◦ 模型生成了一个看似流利但包含**幻觉**的回答：“55% 的通勤者使用汽车，23% 使用公共交通……”,。

    ◦**SPIN 处理** ：此时，SPIN 将人类的诚实回答设为优选，将这个带幻觉的百分比数据设为拒绝，并据此进行微调。

  •  **迭代 1 的模型表现** ：

    ◦ 经过博弈训练后，模型不再胡编具体的百分比。它生成了一个更稳健的定性总结，描述了巴士、火车和渡轮等交通工具，并提到了自行车基础设施,。

    ◦**结果** ：相比迭代 0，迭代 1 的回答在避开幻觉的同时，比原始的人类答案提供了更多准确的细节，更贴近目标数据分布。

   **总结** ：SPIN 通过不断让模型 **超越前一版本的自己** ，在不获取新数据的情况下，充分释放了 SFT 数据中蕴含的潜力

##### Evol-instruct

提示分布的选择（深度优化，广度优化）

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=NGI2YTkxYmNhNDM4YmU4ZWU5NjFlMmE5N2UwZGFkZjhfMkxVazBEdFVvUWVzdmV2V3FJOGpoenp6WFNzSGpBWW5fVG9rZW46UVQ5T2Jld3JGb0hMQm54RGsyT2NEQ3MxbjFiXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=Yjg0OWUxMzdkMjA0ZTFhOGVlNGUzYTA1ODA3M2JjMmRfelQ5eXZ2YmZ3WkpLVHZIdzYwR1V3dTVqY3BvU0U3bEJfVG9rZW46SDhBMmJtYUExb1BzM2x4MzlMSWNybEdwbldjXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

Evol_instruct的数据生成过程

  这篇论文提出了一种名为 **Evol-Instruct** 的新方法，旨在利用大语言模型（LLM）自动大规模生成具有不同复杂程度的开放域指令数据，以提升模型遵循复杂指令的能力。

  以下是该论文中**数据生成过程**的详细说明：

1. 数据生成过程

  数据生成的整体流程被称为一个“进化”流水线，主要包含以下核心步骤：

  •  **初始化指令池** ：从一个基础数据集开始（如论文中使用的 52k 条 Alpaca 数据），作为第 0 代指令池  *D* (0)。

  •  **指令进化（Instruction Evolving）** ：从指令池中取出上一代的指令，通过特定的提示词（Prompt）让 LLM 将其重写。进化分为两个方向：

    ◦**深度进化（In-Depth Evolving）** ：通过五种操作增加指令的难度和复杂度： **增加约束** 、 **深化** 、 **具体化** 、 **增加推理步骤** 、 **复杂化输入** 。

    ◦**广度进化（In-Breadth Evolving）** ：即“变异”，要求模型基于给定指令创造一个全新的、更长尾（罕见）的指令，以提升主题覆盖面和多样性。

  •  **响应生成（Response Generation）** ：对于进化成功后的新指令，使用相同的 LLM（如 ChatGPT）生成对应的详细回答。

  •  **消除进化（Elimination Evolving）** ：这是一个过滤机制，旨在剔除进化失败的样本。失败的情况包括：新指令没有信息增益（与原指令太像）、模型无法回答（回复中包含“sorry”）、回答仅含标点符号，或指令中包含了进化提示词中的冗余文本。

  •  **迭代与合并** ：上述过程会重复进行多轮（论文中执行了 4 轮迭代）。最后，将初始指令与各轮进化产生的所有数据合并，构建最终的微调数据集。

---

2. 迭代进化示例

  论文通过一个简单的初始指令 **“1+1=?”** 展示了 Evol-Instruct 的迭代过程：

  •  **初始指令 (** *C*0 **)** ：`1+1=?`。

  •  **深度进化示例（向更复杂的问题进化）** ：

    ◦**增加推理步骤** ：可能会进化为关于代数的问题，例如：`如果 x^3 + 2x + 3 = 7，x 的值是多少？`。

    ◦**增加约束/深化** ：可能会要求在特定数学猜想下探讨，例如：`如何在哥德巴赫猜想中证明 1+1=2？`。

    ◦**复杂化输入** ：可能会要求模型处理包含公式或表格的代码片段，计算复杂的倒数或开方，如：`1/(sqrt(x) + x^2) = ?`。

  •  **广度进化示例（向更广泛的主题变异）** ：

    ◦ 基于初始的数学/科学逻辑，模型可能会变异出全新的领域指令，例如：`真空中的光速是多少？` 或 `请解释植物光合作用公式中叶绿素的主要作用。`。

  **详细迭代路径：** 随着迭代轮数的增加，指令的平均难度和复杂度会显著提升。根据论文中的分析，从 *C*1 到 *C*4 轮，指令的 **难度评分从 5.48 逐步增长到 7.08** ，这证明了 Evol-Instruct 能够通过不断的“自我重写”将简单任务转化为极具挑战性的复杂任务

##### Task  A  Step Back

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=N2Y4NzBmMjAwMjJlZGQwMTg2N2VlMTE2ZTJkODc2NzhfcmNWNGtEeWFWTWg5RkpsV09mb0Vma080d0hhdTZ0emFfVG9rZW46TzhyQWJiRXZSb2FrNFl4TDM2OWNtU0o2bjdkXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

task_a_step_back 示例

  以下是该研究中数据（即中间推理步骤）生成的具体过程及示例：

1. 数据生成（提示词）过程

  该过程分为两个核心阶段，旨在将包含大量细节的复杂任务分解为更高层级的原则：

  •  **第一步：抽象（Abstraction）** ：

    ◦ 模型不直接回答原始问题，而是被提示去提出一个关于更高层级概念或原理的“**后退一步的问题（Step-back Question）** ”。

    ◦ 这一步利用了“**少样本示例演示（Few-shot exemplar demonstrations）** ”，向模型展示如何从具体实例中提取通用原则。

    ◦ 模型随后回答这个抽象问题，获取相关的“**第一性原理** ”或“ **高层事实** ”。

  •  **第二步：基于抽象的推理（Reasoning）** ：

    ◦ 模型利用第一步获取的高层知识（如物理公式或人物背景）作为依据，来推导原始问题的答案。

    ◦ 在知识密集型任务中，模型还会将“后退一步的问题”用于**检索增强（RAG）** ，以获取更可靠的事实支持。

---

2. 迭代过程详细示例

  论文通过多个领域（STEM、知识问答）展示了这一过程的实际应用：

  示例一：物理学问题（MMLU Physics）

  •  **原始问题** ：如果理想气体的温度增加 2 倍，体积增加 8 倍，压强 P 会发生什么变化？

  •  **第一步（抽象）** ：

    ◦**后退一步的问题** ：“这个问题背后的物理原理是什么？”

    ◦**生成的抽象答案** ：检索/回想起 **理想气体定律** ： *PV* = *nRT* ，并明确各字母代表的物理量。

  •  **第二步（推理）** ：

    ◦ 基于上述定律进行代数运算：将 2*T* 和 8*V* 代入公式得到  *P* (8 *V* )= *nR* (2 *T* )。

    ◦ 得出结论：压强减小了 4 倍。

    ◦**对比** ：如果不使用该方法，模型容易在计算中间步骤出错，甚至得出错误倍数（如 16 倍）。

  示例二：知识问答（TimeQA）

  •  **原始问题** ：“Estella Leopold 在 1954 年 8 月到 1954 年 11 月期间在哪个学校上学？”

  •  **第一步（抽象）** ：

    ◦**后退一步的问题** ：“Estella Leopold 的**教育历史**是什么？”

    ◦**生成的抽象答案** ：模型列出她的完整求学经历：1948 年威斯康星大学学士，1950 年加州大学伯克利分校硕士，1955 年耶鲁大学博士。

  •  **第二步（推理）** ：

    ◦ 根据时间线分析：既然她 1951 年到 1955 年在耶鲁读博士，那么 1954 年底她很可能在耶鲁大学。

    ◦**结果** ：成功修正了基准模型可能因无法直接检索到细碎时间点而产生的错误。

  总结

  该论文证明，通过提示模型先生成 **抽象问题与答案** （即中间数据），可以显著减少模型在处理细节信息时的推理偏差，并在 STEM 任务和知识问答中分别提升了 7%-11% 和 27% 的性能

#### 总结

prompt : 由简单self-instruct -> 复杂的进化学习

response : 简单蒸馏 -> cot蒸馏 -> 多模型蒸馏

### 数据规模

LIMA（至少2k个示例可以提高模型稳定性，7B模型）  DEITA  少数据集，高质量

16K, 性能趋于平稳

10k 高质量

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=NzgxMDJmMzk3MDFkMWJmMzI0MDY4NDc2OTk3ZDI2N2VfdUkweU9EM01mUWJ4Z1dVZVRUQlN6c25JaWRJdzd1aHpfVG9rZW46WmdNTGJ3M0tGb2NweHF4RlBnZ2N3RlFTbkZjXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

### 数据质量

1、输入过滤

  a、ROUGE-L 小于0.7放到指令池里面，我们还排除了包含某些特定关键词（例如，image、picture、graph）的指令，这些关键词通常无法被语言模型处理。

  b、在为每个指令生成新实例时，我们过滤掉了完全相同或输入相同但输出不同的实例。

  c、根据启发式方法（例如，指令过长或过短，实例输出是输入的重复）识别并过滤掉无效生成。

2、输出过滤

  过滤到一些重复性的套路模板的话。比如回复中开头老是说，这是一个很好的问题

3、多样性

  任务的类型,即数据的用途

  数据形式

  Prompt 长度多样性

  Answer 长多样性

    prompt和answer的多样性

    多轮对话

  4、高质量

IFD 指标   有指令时做对的概率” 比 “没指令时蒙对的概率

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=NjZiMzcxYzA3NzEyOThlZjg1ZjllNmFlMGU5YTJjNTFfbExvMUFmYTVhYldxMng1RWF1NXMzMFVxbXFMN0ZJZDdfVG9rZW46RnpIdGIzMFJRb0hmSWV4QTUzV2MzNlA2bjVlXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

### 数据评估

对话能力（谈判角度）

  对话合理性

  对话完整性

债务回收（催收员）：

成功回收率

回收率

收款效率

催收员：

债务回收率：多少债务人最终能把钱全部还完

回收效率：平均每个债务人最终还了多少比例的债务

债务人：

债务人财务健康状况

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=MDRlNTgyZTBhMzAwZjI2ZWEzMjI5YTJmNTliNDhmYTdfOFlYZTFCWkZVSVduN1hxYlM0NktEMlBsV1BvbWNHWkZfVG9rZW46U0xVQWI0NWRqb29TRDh4aW1udWNSZlk3blBoXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=OTYyMWIxODg3ZDEzNzk4YzM0ZDZhZjczYThiMmY1ODhfMlNpOU9CNXdWMGluSllMcktHU1BjTllBZFNQaWFPSU5fVG9rZW46STk4SmJjZ1Rxb2JZdXl4T0xYVWNOQUtobjJjXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

### 数据选择

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDc1YTAzODEzNThkMDg4MjJiZTI2MjNmN2UyYzJmZTRfdkVDT2k2ZlFhNlduRjJTVkNWcXpWaVRta2hxNGd3enBfVG9rZW46SzBvZGJPOHRob2xKZ1h4OHFTUWNuUU1VbmZjXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=MzZlMTgyYjdkMWM5NGQzNjllODU5ZjYyYmM4ODZmZWNfVk52WTVDOGtXZmpQRlJ6QTRFSEpmZmUxQzh0VncwblNfVG9rZW46WVc2MGJDSGxqb0tUeVZ4b3FURmNkbGlTbnBoXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=OGJjODQ4NmVjZmUyNDQ1MGI2MGMyNGY1NGI4M2M3ODRfNW9rVkJGQzNyQm1zRnUyMlN6SFBQRnhqeTB5SzRneDVfVG9rZW46QnQ3MGJDSEZxb1c2bnV4V0JzV2N6Mk1zbnVkXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

## 微调方法和框架

框架ms-swift  https://github.com/modelscope/ms-swift/blob/main/README_CN.md

[llama-factory@志豪](https://mcnm07fm1kb6.feishu.cn/wiki/MvW7wT3pYik4zuk1uN0cyQpcngb)

[ms-swift@于健](https://mcnm07fm1kb6.feishu.cn/wiki/MvLNwaJfQiUXr1k77ircFF9Nnrf)

# 行业和对手

## 国内

## 国外

salient

# 我们自己

## 数据集构造：

### 通用数据(有的太老了，不考虑）：

  Modelscope官方提供 ：https://www.modelscope.cn/datasets/swift/Chinese-Qwen3-235B-2507-Distill-data-110k-SFT

  Modelscope官方提供 ： https://www.modelscope.cn/datasets/swift/Chinese-Qwen3-235B-Thinking-2507-Distill-data-110k-SFT

  Chinese-DeepSeek-R1-Distill-data-110k（单轮） :

  https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k

  https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT    deepseek_R1生成的数据类型* Math：共计36568个样本，

* Exam：共计2432个样本，
* STEM：共计12648个样本，
* General：共计58352，包含弱智吧、逻辑推理、小红书、知乎、Chat等。

  BELLE-data-1.5M（单轮对话）:https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M

  https://github.com/LianjiaTech/BELLE/blob/main/data/awesome_open_instruct_data_for_chinese.md(持续更新）

  Alpaca-GPT-4（单轮对话）:[https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)

  Alpaca-CoT:https://github.com/PhoebusSi/Alpaca-CoT https://huggingface.co/datasets/QingyiSi/AlpacaCoT

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDU3ZjhkYTMxZmNjYTVjNzVlMTIyMzc0YTM4NTgyYjBfWTdrSkRxMzE0YXVqZGxhYTh1eVh2Mzdnbk9FS0RLcW1fVG9rZW46TGg3OWJEZE1Db2NmazZ4MG92MGNSWm56bnpkXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

  alpaca_chinese_dataset :[https://github.com/hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset)

  firefly-train-1.1M(单轮，有类别）: https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M

![](https://mcnm07fm1kb6.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTFhNzhhZjM0YzE1MDM3NTU3N2Y2MzY4ZTgyOTM1YzBfNkVaUGhSZVhnajRESXFlSlNJQWtBN1JWRDdFaEtrbDZfVG9rZW46Q3pTN2JLd2VnbzBzck94UFNrcGNPQlh2bkxnXzE3NzA3Nzc5MTc6MTc3MDc4MTUxN19WNA)

  generated_chat_0.4M（ **不同角色多轮对话数据** ）:https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M

  shibing624/sharegpt_gpt4(多轮对话语料) https://huggingface.co/datasets/shibing624/sharegpt_gpt4

### 业务数据：

    a、从催收agent的角度，ask、reject 、 accept，构造数据

    b、用户角色，还款根因角度，构造数据

    c、真实用户或真实人机的数据

    d、噪音的设计

## 模型选型

qwen 30b-a3b

# 参考

## Github

https://github.com/HqWu-HITCS/Awesome-Chinese-LLM

https://github.com/mbzuai-oryx/Awesome-LLM-Post-training

https://github.com/lmmlzn/Awesome-LLMs-Datasets/

https://github.com/yizhongw/self-instruct?spm=5176.28103460.0.0.96a0755134SHzi

https://github.com/Hannibal046/Awesome-LLM

## 综述

A SURVEY ON POST-TRAINING OF LARGE LANGUAGE MODELS  https://arxiv.org/pdf/2503.06072

Instruction Tuning for Large Language Models: A Survey  https://arxiv.org/pdf/2308.10792

## 数据

SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions  --https://arxiv.org/pdf/2212.10560

SELF-ALIGNMENT WITH INSTRUCTION BACKTRANSLATION   https://arxiv.org/pdf/2308.06259

Training language models to follow instructions with human feedback.(InstructGPT)  https://arxiv.org/abs/2203.02155

Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models https://arxiv.org/pdf/2401.01335

WizardLM: EMPOWERING LARGE PRE-TRAINED LANGUAGE MODELS TO FOLLOW COMPLEX INSTRUCTIONS  https://arxiv.org/pdf/2304.12244

AKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS   https://arxiv.org/pdf/2310.06117

Datasets for Large Language Models: A Comprehensive Survey https://arxiv.org/pdf/2402.18041

## sft

Apple Intelligence Foundation Language Models https://arxiv.org/pdf/2407.21075

* Yi: Open Foundation Models by 01.AI  https://arxiv.org/abs/2403.04652

Lima: Less is more for alignment  https://arxiv.org/pdf/2305.11206

## 领域模型参考

FINANCEBENCH: A New Benchmark for Financial Question Answering https://arxiv.org/abs/2311.11944

LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging    https://arxiv.org/pdf/2209.09900

DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning https://arxiv.org/pdf/2310.15205

FinGPT: Open-Source Financial Large Language Models https://arxiv.org/abs/2306.06031

XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters  https://arxiv.org/pdf/2305.12002

Credit C-GPT: A Domain-Specialized Large Language Model for

Conversational Understanding in Vietnamese Debt Collection https://arxiv.org/pdf/2601.10167

*Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent https://arxiv.org/pdf/2502.18228

## 其他：

https://www.volcengine.com/docs/82379/1221664?lang=zh

https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/142830361
