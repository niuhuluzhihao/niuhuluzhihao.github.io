---
layout:     post
title:      "机器学习算法"
date:       2024-08-12 23:10:00
author:     "NiuHuLu"
catalog: false
published: true
header-style: text
tags:
  - 机器学习
---


# 常用的基本概念和函数
## 信息量
描述 “单个事件” 的意外程度

### 熵
熵则描述 “整个系统” 的不确定性总和
熵（Entropy） 是衡量随机变量（或概率分布）不确定性、混乱程度或信息量的核心指标。  
它的本质是：分布的不确定性越高（结果越难预测），熵值越大；反之，不确定性越低（结果越确定），熵值越小。
$$
H(p) = - \sum_{i=1}^{C} p(x_i) \log p(x_i)
$$


### softmax
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
$$

将一个 K 维的实数向量 转换为 K 维的概率向量，满足：
- 每个元素的值在 [0, 1] 之间（符合概率的取值范围）；- 
所有元素的和为 1（满足概率分布的性质）。

期望使指数差距没有那么大，使用一个超参数T，控制差距
$$
\text{softmax}(z_i,T) = \frac{e^{z_i/t}}{\sum_{j=1}^K e^{z_j/t}}
$$
T约小，则差异越大，硬选择
T越大，则差异越小，软选择


### 交叉熵损失函数
&emsp;核心作用是衡量模型预测**概率分布与真实标签**分布之间的 “差异程度”—— 差异越小，损失值越低，模型预测越准确。
$$
H(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

熵和交叉熵的区别
### 区别与联系

| 概念   | 公式 | 含义 |
|--------|------|------|
| **熵** | \(H(p) = - \sum p(x) \log p(x)\) | 衡量一个分布本身的不确定性 |
| **交叉熵** | \(H(p,q) = - \sum p(x) \log q(x)\) | 衡量真实分布 \(p\) 与预测分布 \(q\) 的差异 |
| **联系** | 若 \(p=q\)，交叉熵 = 熵 | 交叉熵是熵的推广 |

## KL散度（Kullback-Leibler Divergence）

KL散度，又称**相对熵**（Relative Entropy），是信息论中用于衡量两个概率分布之间差异的一种**非对称性度量**。  
它描述了：用一个概率分布 $Q$ 来近似另一个真实概率分布 $P$ 时所“损失”的信息量。

---

### 定义

对于两个离散概率分布 $P$ 和 $Q$，定义在同一个样本空间上，KL散度定义为：

$$
D_{KL}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

对于连续分布，用积分形式表示：

$$
D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} \, dx
$$

其中：
- $P$：真实分布（目标分布）
- $Q$：近似分布（模型分布）
- $\log$ 通常以自然对数（底为 $e$）或以 2 为底（取决于应用场景）

---

### 直观理解

你可以把 KL 散度想象成：

> **如果我误以为数据服从分布 $Q$，但实际上它服从 $P$，那么我平均需要多花多少“比特”或“纳特”的信息来编码这些数据？**

换句话说，KL散度衡量的是：用错误的分布 $Q$ 来编码来自真实分布 $P$ 的数据，会额外损失多少信息。

---

### 性质

- **非负性（Non-negativity）**

  $$
  D_{KL}(P \parallel Q) \ge 0
  $$

  等号成立当且仅当 $P = Q$ 几乎处处成立。

- **不对称性（Asymmetry）**

  $$
  D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)
  $$

  KL 散度不是距离函数（不满足对称性和三角不等式）。

- **差异解释**
  - $D_{KL}(P \parallel Q)$：惩罚 $Q$ 在 $P$ 高概率区域取值过低（欠覆盖）
  - $D_{KL}(Q \parallel P)$：惩罚 $Q$ 在 $P$ 低概率区域取非零值（过覆盖）

---

### 与熵和交叉熵的关系

$$
D_{KL}(P \parallel Q) = H(P, Q) - H(P)
$$

其中：
- $H(P) = -\sum_x P(x)\log P(x)$ 是熵（不确定性）
- $H(P, Q) = -\sum_x P(x)\log Q(x)$ 是交叉熵（用 $Q$ 编码 $P$ 所需的平均比特数）

所以：
- **KL 散度 = 交叉熵 - 熵**
- 表示“因使用错误分布 $Q$ 而增加的编码长度”。

---

### 举例说明

真实硬币分布：  
$$
P = [0.9, 0.1]
$$

假设错误地认为硬币是公平的：  
$$
Q = [0.5, 0.5]
$$

计算 KL 散度：

$$
\begin{aligned}
D_{KL}(P \parallel Q) &= 0.9 \log \frac{0.9}{0.5} + 0.1 \log \frac{0.1}{0.5} \\
&= 0.9 \log(1.8) + 0.1 \log(0.2) \\
&\approx 0.9 \times 0.5878 + 0.1 \times (-1.6094) \\
&\approx 0.368 \;\; \text{nats}
\end{aligned}
$$

解释：用公平硬币模型去编码真实偏置硬币的数据，平均每个结果要多用约 $0.368$ 纳特的信息。

---

### 注意事项

- KL散度不是距离（不对称，不满足三角不等式）
- 如果 $Q(x) = 0$ 且 $P(x) > 0$，则 KL 散度为无穷大
- 实际优化中常使用对称版本（如 **JS散度**）或双向 KL

---

### 总结

KL 散度衡量的是：  
**用分布 $Q$ 去近似真实分布 $P$ 时，所付出的“信息代价”。**

它是机器学习、统计建模和贝叶斯推断中非常核心的工具。


## MSE

# MAE

## RMSE

## bias

## ICC

## R2

## spearman

## pearson

## qwk

## Kendall's W

## Avg Kendall Tau


## Avg Spearman